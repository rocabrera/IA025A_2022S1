{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rocabrera/IA025A_2022S1/blob/master/ex07/Aula_7_Exerc%C3%ADcio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOdQB41_4ZxG",
    "outputId": "5cda0b2a-7edf-49c7-92f8-1bd64f7152e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Rodrigo Cabrera Castaldoni\n"
     ]
    }
   ],
   "source": [
    "nome = \"Rodrigo Cabrera Castaldoni\"\n",
    "print(f'Meu nome é {nome}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IbuChoAPMEn"
   },
   "source": [
    "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_DBb0-Klwf2"
   },
   "source": [
    "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
    "\n",
    "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
    "\n",
    "Alguns conselhos úteis:\n",
    "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
    "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
    "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3twP0YJC4jmJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnyhJZtTRNMx"
   },
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qlIOVCajPWcU"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w9f3PfifAwpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 21:14:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:26:00.0  On |                  N/A |\n",
      "|  0%   43C    P8    18W / 175W |    503MiB /  8192MiB |     31%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       871      G   /usr/lib/Xorg                     206MiB |\n",
      "|    0   N/A  N/A      5544      G   ...AAAAAAAAA= --shared-files      125MiB |\n",
      "|    0   N/A  N/A     11369      G   /usr/lib/firefox/firefox          141MiB |\n",
      "|    0   N/A  N/A     25574      G   ...AAAAAAAAA= --shared-files       26MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "whTCe2i7AtoV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZfxgV2DUk58"
   },
   "source": [
    "## Implementação do MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n_xhKm1EZ3bQ"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "\n",
    "def tokenize(text: str, tokenizer):\n",
    "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
    "\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
    "        # Escreva seu código aqui\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_size = context_size\n",
    "        self._dataset_tokenizer()\n",
    "        \n",
    "    def _dataset_tokenizer(self):\n",
    "        \n",
    "        arrays = np.concatenate([sliding_window_view(tokenize(text, self.tokenizer), self.context_size+1)\n",
    "                                 for text in self.texts]) \n",
    "        \n",
    "        self.data_ids = torch.LongTensor(arrays[:, :-1])\n",
    "        self.target_ids = torch.LongTensor(arrays[:, -1])\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Escreva seu código aqui\n",
    "        return len(self.target_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Escreva seu código aqui\n",
    "        return self.data_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wew-gFbWeBTq"
   },
   "source": [
    "## Teste se sua implementação do MyDataset está correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8r7jBFFUeApe"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8r7jBFFUeApe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    }
   ],
   "source": [
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
    "assert len(dummy_dataset) == 5\n",
    "print('passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[ 3396, 10303,   125],\n",
    "     [ 1660,  5971,   785],\n",
    "     [ 5971,   785,   125],\n",
    "     [  785,   125,  1847],\n",
    "     [  125,  1847, 13779]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
    "\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LfrHHouleJ0"
   },
   "source": [
    "# Carregamento do dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2vFWjsSkmop"
   },
   "source": [
    "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vGlN1WqrXPA6"
   },
   "outputs": [],
   "source": [
    "#!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gxa_4gmiA-wE"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "valid_examples = 100\n",
    "test_examples = 100\n",
    "texts = open('sample_brwac.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gxa_4gmiA-wE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating for debugging purposes.\n"
     ]
    }
   ],
   "source": [
    "print('Truncating for debugging purposes.')\n",
    "texts = texts#[:500]  \n",
    "\n",
    "training_texts = texts[:-(valid_examples + test_examples)]\n",
    "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
    "test_texts = texts[-test_examples:]\n",
    "\n",
    "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KCSGJ5m7py4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 27675945\n",
      "valid examples: 82070\n",
      "test examples: 166726\n"
     ]
    }
   ],
   "source": [
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hGaAjYDfWdd1"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            context_size (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "        \"\"\"\n",
    "        # Escreva seu código aqui.\n",
    "        \n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = torch.nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            torch.nn.Linear(self.context_size*self.embedding_dim, self.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, context_size)\n",
    "        \"\"\"\n",
    "        # Escreva seu código aqui.\n",
    "        input_embeddings = self.embeddings(inputs)\n",
    "        reshaped_input = input_embeddings.flatten(start_dim=1)\n",
    "        \n",
    "        return self.linear_layers(reshaped_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm6_PTH2i98e"
   },
   "source": [
    "## Teste o modelo com um exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RwnxfZlrZoT_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 29794])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_size=128,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "model(sample_train_gpu).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "I3Vh6B-VkA01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 5824098\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of model parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nhbUVsYnVAp"
   },
   "source": [
    "## Assert da Perplexidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gbMP8VAUncfX"
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def perplexity(logits, target):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity.\n",
    "    \"\"\"\n",
    "    # Escreva seu código aqui.\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    sample_loss = - torch.log(probs.gather(1, target.view(-1,1)))\n",
    "    return torch.exp(torch.mean(sample_loss))\n",
    "\n",
    "    \n",
    "n_examples = 1000\n",
    "\n",
    "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "target_token_ids = target_token_ids.to(device)\n",
    "logits = model(sample_train_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gbMP8VAUncfX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my perplexity:              30740\n",
      "correct initial perplexity: 29794\n",
      "Passou o no assert da perplexidade\n"
     ]
    }
   ],
   "source": [
    "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
    "\n",
    "print(f'my perplexity:              {int(my_perplexity)}')\n",
    "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
    "\n",
    "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
    "print('Passou o no assert da perplexidade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiJtrsqPnE_l"
   },
   "source": [
    "## Laço de Treinamento e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zIMSaY-UUGUE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train ppl: 30495.82, valid ppl: 30543.47\n",
      "5000 steps; 320000 examples so far; train ppl: 3045.68, valid ppl: 1877.71\n",
      "10000 steps; 640000 examples so far; train ppl: 1703.93, valid ppl: 1638.80\n",
      "15000 steps; 960000 examples so far; train ppl: 1551.20, valid ppl: 1540.97\n",
      "20000 steps; 1280000 examples so far; train ppl: 1477.50, valid ppl: 1462.10\n",
      "25000 steps; 1600000 examples so far; train ppl: 1400.56, valid ppl: 1385.39\n",
      "30000 steps; 1920000 examples so far; train ppl: 1325.40, valid ppl: 1310.93\n",
      "35000 steps; 2240000 examples so far; train ppl: 1267.89, valid ppl: 1241.31\n",
      "40000 steps; 2560000 examples so far; train ppl: 1198.58, valid ppl: 1178.89\n",
      "45000 steps; 2880000 examples so far; train ppl: 1135.71, valid ppl: 1122.71\n",
      "50000 steps; 3200000 examples so far; train ppl: 1088.78, valid ppl: 1074.64\n",
      "55000 steps; 3520000 examples so far; train ppl: 1042.42, valid ppl: 1034.97\n",
      "60000 steps; 3840000 examples so far; train ppl: 1006.62, valid ppl: 999.33\n",
      "65000 steps; 4160000 examples so far; train ppl: 968.70, valid ppl: 969.06\n",
      "70000 steps; 4480000 examples so far; train ppl: 935.55, valid ppl: 940.57\n",
      "75000 steps; 4800000 examples so far; train ppl: 907.99, valid ppl: 915.70\n",
      "80000 steps; 5120000 examples so far; train ppl: 890.17, valid ppl: 892.68\n",
      "85000 steps; 5440000 examples so far; train ppl: 866.38, valid ppl: 874.22\n",
      "90000 steps; 5760000 examples so far; train ppl: 855.80, valid ppl: 855.72\n",
      "95000 steps; 6080000 examples so far; train ppl: 837.00, valid ppl: 838.70\n",
      "100000 steps; 6400000 examples so far; train ppl: 817.08, valid ppl: 822.03\n",
      "105000 steps; 6720000 examples so far; train ppl: 802.17, valid ppl: 807.23\n",
      "110000 steps; 7040000 examples so far; train ppl: 787.30, valid ppl: 792.92\n",
      "115000 steps; 7360000 examples so far; train ppl: 772.95, valid ppl: 779.84\n",
      "120000 steps; 7680000 examples so far; train ppl: 765.25, valid ppl: 768.44\n",
      "125000 steps; 8000000 examples so far; train ppl: 750.91, valid ppl: 757.71\n",
      "130000 steps; 8320000 examples so far; train ppl: 736.36, valid ppl: 746.51\n",
      "135000 steps; 8640000 examples so far; train ppl: 725.28, valid ppl: 736.75\n",
      "140000 steps; 8960000 examples so far; train ppl: 721.77, valid ppl: 726.47\n",
      "145000 steps; 9280000 examples so far; train ppl: 715.75, valid ppl: 717.67\n",
      "150000 steps; 9600000 examples so far; train ppl: 701.60, valid ppl: 708.60\n",
      "155000 steps; 9920000 examples so far; train ppl: 689.62, valid ppl: 700.65\n",
      "160000 steps; 10240000 examples so far; train ppl: 682.92, valid ppl: 691.75\n",
      "165000 steps; 10560000 examples so far; train ppl: 681.50, valid ppl: 683.38\n",
      "170000 steps; 10880000 examples so far; train ppl: 665.09, valid ppl: 676.61\n",
      "175000 steps; 11200000 examples so far; train ppl: 664.02, valid ppl: 669.93\n",
      "180000 steps; 11520000 examples so far; train ppl: 651.22, valid ppl: 663.96\n",
      "185000 steps; 11840000 examples so far; train ppl: 653.62, valid ppl: 656.60\n",
      "190000 steps; 12160000 examples so far; train ppl: 646.63, valid ppl: 650.96\n",
      "195000 steps; 12480000 examples so far; train ppl: 637.75, valid ppl: 644.79\n",
      "200000 steps; 12800000 examples so far; train ppl: 631.09, valid ppl: 638.12\n",
      "205000 steps; 13120000 examples so far; train ppl: 623.06, valid ppl: 632.65\n",
      "210000 steps; 13440000 examples so far; train ppl: 617.61, valid ppl: 627.81\n",
      "215000 steps; 13760000 examples so far; train ppl: 615.46, valid ppl: 621.60\n",
      "220000 steps; 14080000 examples so far; train ppl: 614.76, valid ppl: 617.48\n",
      "225000 steps; 14400000 examples so far; train ppl: 605.46, valid ppl: 612.59\n",
      "230000 steps; 14720000 examples so far; train ppl: 604.45, valid ppl: 608.17\n",
      "235000 steps; 15040000 examples so far; train ppl: 595.67, valid ppl: 604.60\n",
      "240000 steps; 15360000 examples so far; train ppl: 591.89, valid ppl: 600.02\n",
      "245000 steps; 15680000 examples so far; train ppl: 589.39, valid ppl: 595.93\n",
      "250000 steps; 16000000 examples so far; train ppl: 585.59, valid ppl: 591.82\n",
      "255000 steps; 16320000 examples so far; train ppl: 580.11, valid ppl: 587.23\n",
      "260000 steps; 16640000 examples so far; train ppl: 578.64, valid ppl: 584.05\n",
      "265000 steps; 16960000 examples so far; train ppl: 572.03, valid ppl: 580.51\n",
      "270000 steps; 17280000 examples so far; train ppl: 572.34, valid ppl: 577.06\n",
      "275000 steps; 17600000 examples so far; train ppl: 566.42, valid ppl: 573.93\n",
      "280000 steps; 17920000 examples so far; train ppl: 567.66, valid ppl: 569.94\n",
      "285000 steps; 18240000 examples so far; train ppl: 557.22, valid ppl: 565.87\n",
      "290000 steps; 18560000 examples so far; train ppl: 555.04, valid ppl: 563.43\n",
      "295000 steps; 18880000 examples so far; train ppl: 557.59, valid ppl: 559.72\n",
      "300000 steps; 19200000 examples so far; train ppl: 556.49, valid ppl: 556.65\n",
      "305000 steps; 19520000 examples so far; train ppl: 548.03, valid ppl: 554.22\n",
      "310000 steps; 19840000 examples so far; train ppl: 547.39, valid ppl: 551.23\n",
      "315000 steps; 20160000 examples so far; train ppl: 540.98, valid ppl: 547.97\n",
      "320000 steps; 20480000 examples so far; train ppl: 538.41, valid ppl: 545.58\n",
      "325000 steps; 20800000 examples so far; train ppl: 531.59, valid ppl: 543.34\n",
      "330000 steps; 21120000 examples so far; train ppl: 529.26, valid ppl: 540.21\n",
      "335000 steps; 21440000 examples so far; train ppl: 530.60, valid ppl: 536.49\n",
      "340000 steps; 21760000 examples so far; train ppl: 529.28, valid ppl: 535.44\n",
      "345000 steps; 22080000 examples so far; train ppl: 525.04, valid ppl: 532.15\n",
      "350000 steps; 22400000 examples so far; train ppl: 524.34, valid ppl: 532.27\n",
      "355000 steps; 22720000 examples so far; train ppl: 527.54, valid ppl: 529.19\n",
      "360000 steps; 23040000 examples so far; train ppl: 519.51, valid ppl: 525.74\n",
      "365000 steps; 23360000 examples so far; train ppl: 518.44, valid ppl: 523.85\n",
      "370000 steps; 23680000 examples so far; train ppl: 518.26, valid ppl: 522.31\n",
      "375000 steps; 24000000 examples so far; train ppl: 512.58, valid ppl: 519.30\n",
      "380000 steps; 24320000 examples so far; train ppl: 506.96, valid ppl: 517.62\n",
      "385000 steps; 24640000 examples so far; train ppl: 505.25, valid ppl: 517.15\n",
      "390000 steps; 24960000 examples so far; train ppl: 501.01, valid ppl: 514.79\n",
      "395000 steps; 25280000 examples so far; train ppl: 504.46, valid ppl: 511.64\n",
      "400000 steps; 25600000 examples so far; train ppl: 500.83, valid ppl: 509.60\n",
      "405000 steps; 25920000 examples so far; train ppl: 500.28, valid ppl: 507.68\n",
      "410000 steps; 26240000 examples so far; train ppl: 500.83, valid ppl: 505.95\n",
      "415000 steps; 26560000 examples so far; train ppl: 497.64, valid ppl: 504.45\n",
      "420000 steps; 26880000 examples so far; train ppl: 493.90, valid ppl: 502.26\n",
      "425000 steps; 27200000 examples so far; train ppl: 496.77, valid ppl: 500.43\n",
      "430000 steps; 27520000 examples so far; train ppl: 496.24, valid ppl: 498.40\n",
      "435000 steps; 27840000 examples so far; train ppl: 478.98, valid ppl: 497.88\n",
      "440000 steps; 28160000 examples so far; train ppl: 469.87, valid ppl: 495.71\n",
      "445000 steps; 28480000 examples so far; train ppl: 473.19, valid ppl: 493.86\n",
      "450000 steps; 28800000 examples so far; train ppl: 471.06, valid ppl: 493.24\n",
      "455000 steps; 29120000 examples so far; train ppl: 474.56, valid ppl: 490.27\n",
      "460000 steps; 29440000 examples so far; train ppl: 471.05, valid ppl: 488.72\n",
      "465000 steps; 29760000 examples so far; train ppl: 466.36, valid ppl: 488.12\n",
      "470000 steps; 30080000 examples so far; train ppl: 468.86, valid ppl: 486.88\n",
      "475000 steps; 30400000 examples so far; train ppl: 469.73, valid ppl: 485.08\n",
      "480000 steps; 30720000 examples so far; train ppl: 469.00, valid ppl: 483.69\n",
      "485000 steps; 31040000 examples so far; train ppl: 464.33, valid ppl: 482.13\n",
      "490000 steps; 31360000 examples so far; train ppl: 462.56, valid ppl: 481.46\n",
      "495000 steps; 31680000 examples so far; train ppl: 459.36, valid ppl: 480.19\n",
      "500000 steps; 32000000 examples so far; train ppl: 459.22, valid ppl: 479.01\n",
      "505000 steps; 32320000 examples so far; train ppl: 464.82, valid ppl: 477.46\n",
      "510000 steps; 32640000 examples so far; train ppl: 459.17, valid ppl: 477.07\n",
      "515000 steps; 32960000 examples so far; train ppl: 458.11, valid ppl: 476.17\n",
      "520000 steps; 33280000 examples so far; train ppl: 456.72, valid ppl: 474.69\n",
      "525000 steps; 33600000 examples so far; train ppl: 456.79, valid ppl: 473.70\n",
      "530000 steps; 33920000 examples so far; train ppl: 453.79, valid ppl: 470.96\n",
      "535000 steps; 34240000 examples so far; train ppl: 453.98, valid ppl: 469.24\n",
      "540000 steps; 34560000 examples so far; train ppl: 456.72, valid ppl: 468.70\n",
      "545000 steps; 34880000 examples so far; train ppl: 455.25, valid ppl: 467.54\n",
      "550000 steps; 35200000 examples so far; train ppl: 453.11, valid ppl: 466.04\n",
      "555000 steps; 35520000 examples so far; train ppl: 450.36, valid ppl: 464.52\n",
      "560000 steps; 35840000 examples so far; train ppl: 453.98, valid ppl: 464.90\n",
      "565000 steps; 36160000 examples so far; train ppl: 451.87, valid ppl: 463.75\n",
      "570000 steps; 36480000 examples so far; train ppl: 455.13, valid ppl: 462.32\n",
      "575000 steps; 36800000 examples so far; train ppl: 447.99, valid ppl: 461.81\n",
      "580000 steps; 37120000 examples so far; train ppl: 445.43, valid ppl: 460.42\n",
      "585000 steps; 37440000 examples so far; train ppl: 447.20, valid ppl: 459.48\n",
      "590000 steps; 37760000 examples so far; train ppl: 447.34, valid ppl: 458.32\n",
      "595000 steps; 38080000 examples so far; train ppl: 442.71, valid ppl: 456.44\n",
      "600000 steps; 38400000 examples so far; train ppl: 446.34, valid ppl: 456.40\n",
      "605000 steps; 38720000 examples so far; train ppl: 441.15, valid ppl: 456.31\n",
      "610000 steps; 39040000 examples so far; train ppl: 448.60, valid ppl: 454.82\n",
      "615000 steps; 39360000 examples so far; train ppl: 439.52, valid ppl: 453.84\n",
      "620000 steps; 39680000 examples so far; train ppl: 442.16, valid ppl: 452.97\n",
      "625000 steps; 40000000 examples so far; train ppl: 437.10, valid ppl: 451.97\n",
      "630000 steps; 40320000 examples so far; train ppl: 439.66, valid ppl: 451.08\n",
      "635000 steps; 40640000 examples so far; train ppl: 436.68, valid ppl: 451.64\n",
      "640000 steps; 40960000 examples so far; train ppl: 439.29, valid ppl: 448.62\n",
      "645000 steps; 41280000 examples so far; train ppl: 433.67, valid ppl: 448.36\n",
      "650000 steps; 41600000 examples so far; train ppl: 434.55, valid ppl: 448.36\n",
      "655000 steps; 41920000 examples so far; train ppl: 433.83, valid ppl: 446.87\n",
      "660000 steps; 42240000 examples so far; train ppl: 434.36, valid ppl: 446.06\n",
      "665000 steps; 42560000 examples so far; train ppl: 435.20, valid ppl: 446.21\n",
      "670000 steps; 42880000 examples so far; train ppl: 436.56, valid ppl: 445.92\n",
      "675000 steps; 43200000 examples so far; train ppl: 435.05, valid ppl: 444.49\n",
      "680000 steps; 43520000 examples so far; train ppl: 433.50, valid ppl: 442.69\n",
      "685000 steps; 43840000 examples so far; train ppl: 434.86, valid ppl: 441.31\n",
      "690000 steps; 44160000 examples so far; train ppl: 432.07, valid ppl: 441.95\n",
      "695000 steps; 44480000 examples so far; train ppl: 430.69, valid ppl: 440.36\n",
      "700000 steps; 44800000 examples so far; train ppl: 427.97, valid ppl: 438.53\n",
      "705000 steps; 45120000 examples so far; train ppl: 428.79, valid ppl: 437.81\n",
      "710000 steps; 45440000 examples so far; train ppl: 428.91, valid ppl: 437.42\n",
      "715000 steps; 45760000 examples so far; train ppl: 426.62, valid ppl: 436.64\n",
      "720000 steps; 46080000 examples so far; train ppl: 427.49, valid ppl: 436.53\n",
      "725000 steps; 46400000 examples so far; train ppl: 422.79, valid ppl: 434.77\n",
      "730000 steps; 46720000 examples so far; train ppl: 428.10, valid ppl: 434.75\n",
      "735000 steps; 47040000 examples so far; train ppl: 425.11, valid ppl: 432.62\n",
      "740000 steps; 47360000 examples so far; train ppl: 421.93, valid ppl: 433.08\n",
      "745000 steps; 47680000 examples so far; train ppl: 422.92, valid ppl: 432.87\n",
      "750000 steps; 48000000 examples so far; train ppl: 425.46, valid ppl: 431.13\n",
      "755000 steps; 48320000 examples so far; train ppl: 417.35, valid ppl: 430.82\n",
      "760000 steps; 48640000 examples so far; train ppl: 422.67, valid ppl: 430.85\n",
      "765000 steps; 48960000 examples so far; train ppl: 418.38, valid ppl: 429.78\n",
      "770000 steps; 49280000 examples so far; train ppl: 416.78, valid ppl: 429.12\n",
      "775000 steps; 49600000 examples so far; train ppl: 421.07, valid ppl: 428.15\n",
      "780000 steps; 49920000 examples so far; train ppl: 417.74, valid ppl: 426.82\n",
      "785000 steps; 50240000 examples so far; train ppl: 419.82, valid ppl: 426.27\n",
      "790000 steps; 50560000 examples so far; train ppl: 415.90, valid ppl: 426.99\n",
      "795000 steps; 50880000 examples so far; train ppl: 417.61, valid ppl: 425.61\n",
      "800000 steps; 51200000 examples so far; train ppl: 416.94, valid ppl: 426.79\n",
      "805000 steps; 51520000 examples so far; train ppl: 414.82, valid ppl: 425.24\n",
      "810000 steps; 51840000 examples so far; train ppl: 417.59, valid ppl: 424.69\n",
      "815000 steps; 52160000 examples so far; train ppl: 417.46, valid ppl: 425.12\n",
      "820000 steps; 52480000 examples so far; train ppl: 417.54, valid ppl: 424.75\n",
      "825000 steps; 52800000 examples so far; train ppl: 419.70, valid ppl: 423.41\n",
      "830000 steps; 53120000 examples so far; train ppl: 414.44, valid ppl: 423.41\n",
      "835000 steps; 53440000 examples so far; train ppl: 416.02, valid ppl: 422.47\n",
      "840000 steps; 53760000 examples so far; train ppl: 414.69, valid ppl: 421.96\n",
      "845000 steps; 54080000 examples so far; train ppl: 416.93, valid ppl: 421.53\n",
      "850000 steps; 54400000 examples so far; train ppl: 410.90, valid ppl: 421.27\n",
      "855000 steps; 54720000 examples so far; train ppl: 411.31, valid ppl: 420.50\n",
      "860000 steps; 55040000 examples so far; train ppl: 413.03, valid ppl: 418.84\n",
      "865000 steps; 55360000 examples so far; train ppl: 409.29, valid ppl: 419.65\n",
      "870000 steps; 55680000 examples so far; train ppl: 401.89, valid ppl: 419.15\n",
      "875000 steps; 56000000 examples so far; train ppl: 399.42, valid ppl: 419.77\n",
      "880000 steps; 56320000 examples so far; train ppl: 401.79, valid ppl: 419.46\n",
      "885000 steps; 56640000 examples so far; train ppl: 400.09, valid ppl: 419.27\n",
      "890000 steps; 56960000 examples so far; train ppl: 402.81, valid ppl: 417.94\n",
      "895000 steps; 57280000 examples so far; train ppl: 398.02, valid ppl: 417.14\n",
      "900000 steps; 57600000 examples so far; train ppl: 399.78, valid ppl: 417.12\n",
      "905000 steps; 57920000 examples so far; train ppl: 400.22, valid ppl: 417.03\n",
      "910000 steps; 58240000 examples so far; train ppl: 404.00, valid ppl: 416.67\n",
      "915000 steps; 58560000 examples so far; train ppl: 399.78, valid ppl: 415.31\n",
      "920000 steps; 58880000 examples so far; train ppl: 401.56, valid ppl: 415.29\n",
      "925000 steps; 59200000 examples so far; train ppl: 398.53, valid ppl: 414.29\n",
      "930000 steps; 59520000 examples so far; train ppl: 398.90, valid ppl: 414.05\n",
      "935000 steps; 59840000 examples so far; train ppl: 397.60, valid ppl: 414.39\n",
      "940000 steps; 60160000 examples so far; train ppl: 401.73, valid ppl: 414.44\n",
      "945000 steps; 60480000 examples so far; train ppl: 400.73, valid ppl: 414.12\n",
      "950000 steps; 60800000 examples so far; train ppl: 399.99, valid ppl: 413.65\n",
      "955000 steps; 61120000 examples so far; train ppl: 395.89, valid ppl: 413.03\n",
      "960000 steps; 61440000 examples so far; train ppl: 400.17, valid ppl: 412.61\n",
      "965000 steps; 61760000 examples so far; train ppl: 395.50, valid ppl: 411.92\n",
      "970000 steps; 62080000 examples so far; train ppl: 394.78, valid ppl: 412.49\n",
      "975000 steps; 62400000 examples so far; train ppl: 398.36, valid ppl: 412.12\n",
      "980000 steps; 62720000 examples so far; train ppl: 402.16, valid ppl: 412.36\n",
      "985000 steps; 63040000 examples so far; train ppl: 398.94, valid ppl: 411.94\n",
      "990000 steps; 63360000 examples so far; train ppl: 400.89, valid ppl: 411.09\n",
      "995000 steps; 63680000 examples so far; train ppl: 401.18, valid ppl: 410.72\n",
      "1000000 steps; 64000000 examples so far; train ppl: 401.55, valid ppl: 409.69\n",
      "1005000 steps; 64320000 examples so far; train ppl: 396.89, valid ppl: 410.48\n",
      "1010000 steps; 64640000 examples so far; train ppl: 397.13, valid ppl: 411.02\n",
      "1015000 steps; 64960000 examples so far; train ppl: 398.48, valid ppl: 410.58\n",
      "1020000 steps; 65280000 examples so far; train ppl: 398.12, valid ppl: 409.07\n",
      "1025000 steps; 65600000 examples so far; train ppl: 394.25, valid ppl: 409.44\n",
      "1030000 steps; 65920000 examples so far; train ppl: 396.38, valid ppl: 410.63\n",
      "1035000 steps; 66240000 examples so far; train ppl: 397.01, valid ppl: 410.51\n",
      "1040000 steps; 66560000 examples so far; train ppl: 395.60, valid ppl: 408.69\n",
      "1045000 steps; 66880000 examples so far; train ppl: 397.50, valid ppl: 408.68\n",
      "1050000 steps; 67200000 examples so far; train ppl: 397.61, valid ppl: 408.86\n",
      "1055000 steps; 67520000 examples so far; train ppl: 394.13, valid ppl: 408.18\n",
      "1060000 steps; 67840000 examples so far; train ppl: 395.29, valid ppl: 407.08\n",
      "1065000 steps; 68160000 examples so far; train ppl: 394.17, valid ppl: 407.66\n",
      "1070000 steps; 68480000 examples so far; train ppl: 399.20, valid ppl: 406.67\n",
      "1075000 steps; 68800000 examples so far; train ppl: 392.65, valid ppl: 405.90\n",
      "1080000 steps; 69120000 examples so far; train ppl: 396.22, valid ppl: 406.39\n",
      "1085000 steps; 69440000 examples so far; train ppl: 392.26, valid ppl: 406.80\n",
      "1090000 steps; 69760000 examples so far; train ppl: 395.19, valid ppl: 405.57\n",
      "1095000 steps; 70080000 examples so far; train ppl: 397.24, valid ppl: 406.50\n",
      "1100000 steps; 70400000 examples so far; train ppl: 395.71, valid ppl: 406.69\n",
      "1105000 steps; 70720000 examples so far; train ppl: 393.28, valid ppl: 405.45\n",
      "1110000 steps; 71040000 examples so far; train ppl: 393.39, valid ppl: 404.42\n",
      "1115000 steps; 71360000 examples so far; train ppl: 395.12, valid ppl: 404.62\n",
      "1120000 steps; 71680000 examples so far; train ppl: 389.83, valid ppl: 404.20\n",
      "1125000 steps; 72000000 examples so far; train ppl: 395.86, valid ppl: 404.53\n",
      "1130000 steps; 72320000 examples so far; train ppl: 397.34, valid ppl: 405.21\n",
      "1135000 steps; 72640000 examples so far; train ppl: 393.11, valid ppl: 405.49\n",
      "1140000 steps; 72960000 examples so far; train ppl: 393.63, valid ppl: 404.35\n",
      "1145000 steps; 73280000 examples so far; train ppl: 395.68, valid ppl: 404.05\n",
      "1150000 steps; 73600000 examples so far; train ppl: 397.09, valid ppl: 405.52\n",
      "1155000 steps; 73920000 examples so far; train ppl: 392.39, valid ppl: 405.37\n",
      "1160000 steps; 74240000 examples so far; train ppl: 397.04, valid ppl: 404.54\n",
      "1165000 steps; 74560000 examples so far; train ppl: 392.78, valid ppl: 404.44\n",
      "1170000 steps; 74880000 examples so far; train ppl: 395.33, valid ppl: 403.15\n",
      "1175000 steps; 75200000 examples so far; train ppl: 399.36, valid ppl: 402.75\n",
      "1180000 steps; 75520000 examples so far; train ppl: 390.88, valid ppl: 402.95\n",
      "1185000 steps; 75840000 examples so far; train ppl: 390.66, valid ppl: 402.53\n",
      "1190000 steps; 76160000 examples so far; train ppl: 391.88, valid ppl: 402.10\n",
      "1195000 steps; 76480000 examples so far; train ppl: 398.89, valid ppl: 401.42\n",
      "1200000 steps; 76800000 examples so far; train ppl: 393.43, valid ppl: 401.83\n",
      "1205000 steps; 77120000 examples so far; train ppl: 396.02, valid ppl: 402.31\n",
      "1210000 steps; 77440000 examples so far; train ppl: 396.61, valid ppl: 401.74\n",
      "1215000 steps; 77760000 examples so far; train ppl: 391.91, valid ppl: 402.85\n",
      "1220000 steps; 78080000 examples so far; train ppl: 391.60, valid ppl: 401.95\n",
      "1225000 steps; 78400000 examples so far; train ppl: 394.68, valid ppl: 401.17\n",
      "1230000 steps; 78720000 examples so far; train ppl: 388.54, valid ppl: 400.50\n",
      "1235000 steps; 79040000 examples so far; train ppl: 391.37, valid ppl: 402.14\n",
      "1240000 steps; 79360000 examples so far; train ppl: 390.42, valid ppl: 401.24\n",
      "1245000 steps; 79680000 examples so far; train ppl: 393.08, valid ppl: 401.98\n",
      "1250000 steps; 80000000 examples so far; train ppl: 397.62, valid ppl: 400.87\n",
      "1255000 steps; 80320000 examples so far; train ppl: 391.85, valid ppl: 400.70\n",
      "1260000 steps; 80640000 examples so far; train ppl: 392.66, valid ppl: 398.45\n",
      "1265000 steps; 80960000 examples so far; train ppl: 391.62, valid ppl: 398.50\n",
      "1270000 steps; 81280000 examples so far; train ppl: 395.44, valid ppl: 399.63\n",
      "1275000 steps; 81600000 examples so far; train ppl: 394.94, valid ppl: 399.44\n",
      "1280000 steps; 81920000 examples so far; train ppl: 395.54, valid ppl: 400.26\n",
      "1285000 steps; 82240000 examples so far; train ppl: 392.47, valid ppl: 400.08\n",
      "1290000 steps; 82560000 examples so far; train ppl: 392.53, valid ppl: 399.85\n",
      "1295000 steps; 82880000 examples so far; train ppl: 390.84, valid ppl: 399.65\n",
      "1300000 steps; 83200000 examples so far; train ppl: 384.60, valid ppl: 398.57\n",
      "1305000 steps; 83520000 examples so far; train ppl: 381.52, valid ppl: 400.11\n",
      "1310000 steps; 83840000 examples so far; train ppl: 385.07, valid ppl: 399.59\n",
      "1315000 steps; 84160000 examples so far; train ppl: 383.00, valid ppl: 399.82\n",
      "1320000 steps; 84480000 examples so far; train ppl: 380.93, valid ppl: 398.94\n",
      "1325000 steps; 84800000 examples so far; train ppl: 381.82, valid ppl: 398.47\n",
      "1330000 steps; 85120000 examples so far; train ppl: 379.48, valid ppl: 398.32\n",
      "1335000 steps; 85440000 examples so far; train ppl: 386.34, valid ppl: 399.40\n",
      "1340000 steps; 85760000 examples so far; train ppl: 386.10, valid ppl: 400.75\n",
      "1345000 steps; 86080000 examples so far; train ppl: 383.54, valid ppl: 399.56\n",
      "1350000 steps; 86400000 examples so far; train ppl: 390.06, valid ppl: 399.56\n",
      "1355000 steps; 86720000 examples so far; train ppl: 385.05, valid ppl: 398.14\n",
      "1360000 steps; 87040000 examples so far; train ppl: 386.39, valid ppl: 399.28\n",
      "1365000 steps; 87360000 examples so far; train ppl: 385.94, valid ppl: 398.03\n",
      "1370000 steps; 87680000 examples so far; train ppl: 382.81, valid ppl: 398.78\n",
      "1375000 steps; 88000000 examples so far; train ppl: 385.93, valid ppl: 397.70\n",
      "1380000 steps; 88320000 examples so far; train ppl: 383.64, valid ppl: 397.82\n",
      "1385000 steps; 88640000 examples so far; train ppl: 382.74, valid ppl: 397.43\n",
      "1390000 steps; 88960000 examples so far; train ppl: 384.61, valid ppl: 397.43\n",
      "1395000 steps; 89280000 examples so far; train ppl: 390.04, valid ppl: 397.95\n",
      "1400000 steps; 89600000 examples so far; train ppl: 384.71, valid ppl: 399.14\n",
      "1405000 steps; 89920000 examples so far; train ppl: 388.03, valid ppl: 397.88\n",
      "1410000 steps; 90240000 examples so far; train ppl: 386.11, valid ppl: 397.74\n",
      "1415000 steps; 90560000 examples so far; train ppl: 384.68, valid ppl: 397.17\n",
      "1420000 steps; 90880000 examples so far; train ppl: 382.89, valid ppl: 396.78\n",
      "1425000 steps; 91200000 examples so far; train ppl: 384.53, valid ppl: 397.03\n",
      "1430000 steps; 91520000 examples so far; train ppl: 387.81, valid ppl: 397.27\n",
      "1435000 steps; 91840000 examples so far; train ppl: 386.76, valid ppl: 397.51\n",
      "1440000 steps; 92160000 examples so far; train ppl: 384.51, valid ppl: 398.18\n",
      "1445000 steps; 92480000 examples so far; train ppl: 386.21, valid ppl: 397.79\n",
      "1450000 steps; 92800000 examples so far; train ppl: 382.72, valid ppl: 399.06\n",
      "1455000 steps; 93120000 examples so far; train ppl: 388.10, valid ppl: 398.93\n",
      "1460000 steps; 93440000 examples so far; train ppl: 389.29, valid ppl: 397.68\n",
      "1465000 steps; 93760000 examples so far; train ppl: 384.98, valid ppl: 398.30\n",
      "1470000 steps; 94080000 examples so far; train ppl: 383.32, valid ppl: 396.99\n",
      "1475000 steps; 94400000 examples so far; train ppl: 384.88, valid ppl: 398.41\n",
      "1480000 steps; 94720000 examples so far; train ppl: 386.57, valid ppl: 397.31\n",
      "1485000 steps; 95040000 examples so far; train ppl: 387.55, valid ppl: 397.17\n",
      "1490000 steps; 95360000 examples so far; train ppl: 389.12, valid ppl: 398.18\n",
      "1495000 steps; 95680000 examples so far; train ppl: 383.89, valid ppl: 397.44\n",
      "1500000 steps; 96000000 examples so far; train ppl: 382.40, valid ppl: 398.02\n",
      "1505000 steps; 96320000 examples so far; train ppl: 390.90, valid ppl: 397.87\n",
      "1510000 steps; 96640000 examples so far; train ppl: 385.14, valid ppl: 397.29\n",
      "1515000 steps; 96960000 examples so far; train ppl: 384.26, valid ppl: 397.48\n",
      "1520000 steps; 97280000 examples so far; train ppl: 385.19, valid ppl: 396.91\n",
      "1525000 steps; 97600000 examples so far; train ppl: 383.54, valid ppl: 396.78\n",
      "1530000 steps; 97920000 examples so far; train ppl: 384.72, valid ppl: 398.32\n",
      "1535000 steps; 98240000 examples so far; train ppl: 388.00, valid ppl: 396.70\n",
      "1540000 steps; 98560000 examples so far; train ppl: 385.90, valid ppl: 396.78\n",
      "1545000 steps; 98880000 examples so far; train ppl: 392.96, valid ppl: 395.72\n",
      "1550000 steps; 99200000 examples so far; train ppl: 384.66, valid ppl: 397.56\n",
      "1555000 steps; 99520000 examples so far; train ppl: 387.12, valid ppl: 396.50\n",
      "1560000 steps; 99840000 examples so far; train ppl: 387.39, valid ppl: 396.67\n"
     ]
    }
   ],
   "source": [
    "max_examples = 100_000_000\n",
    "eval_every_steps = 5000\n",
    "lr = 3e-5\n",
    "\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_size=256,\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train_step(input, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    logits = model(input.to(device))\n",
    "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input, target):\n",
    "    logits = model(input)\n",
    "    loss = nn.functional.cross_entropy(logits, target)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "while n_examples < max_examples:\n",
    "    for input, target in train_loader:\n",
    "        loss = train_step(input.to(device), target.to(device)) \n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(input.to(device), target.to(device))\n",
    "                    for input, target in validation_loader]))\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(input)  # Increment of batch size\n",
    "        step += 1\n",
    "        if n_examples >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgdNymJdNPXP"
   },
   "source": [
    "## Avaliação final no dataset de teste\n",
    "\n",
    "\n",
    "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nxN5YytzZ7Tn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 366.3844278071844\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target in test_loader\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHvEs8mPszy_"
   },
   "source": [
    "## Teste seu modelo com uma sentença\n",
    "\n",
    "Escolha uma sentença gerada pelo modelo que ache interessante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-CFElf4tsytW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O que o gato disse para o cachorro? O\n",
      "O que o gato disse para o cachorro? O que\n",
      "O que o gato disse para o cachorro? O que é\n",
      "O que o gato disse para o cachorro? O que é o\n",
      "O que o gato disse para o cachorro? O que é o que\n",
      "O que o gato disse para o cachorro? O que é o que é\n",
      "O que o gato disse para o cachorro? O que é o que é o\n",
      "O que o gato disse para o cachorro? O que é o que é o que\n",
      "O que o gato disse para o cachorro? O que é o que é o que é\n",
      "O que o gato disse para o cachorro? O que é o que é o que é o\n"
     ]
    }
   ],
   "source": [
    "prompt = 'O que o gato disse para o cachorro?'  # Ex: 'Eu gosto de comer pizza pois me faz'\n",
    "max_output_tokens = 10\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
    "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
    "    prompt = tokenizer.decode(input_ids)\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Aula 7 - Exercício",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_huggingface",
   "language": "python",
   "name": "venv_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
