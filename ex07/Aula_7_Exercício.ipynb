{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rocabrera/IA025A_2022S1/blob/master/ex07/Aula_7_Exerc%C3%ADcio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOdQB41_4ZxG",
    "outputId": "5cda0b2a-7edf-49c7-92f8-1bd64f7152e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Rodrigo Cabrera Castaldoni\n"
     ]
    }
   ],
   "source": [
    "nome = \"Rodrigo Cabrera Castaldoni\"\n",
    "print(f'Meu nome é {nome}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IbuChoAPMEn"
   },
   "source": [
    "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_DBb0-Klwf2"
   },
   "source": [
    "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
    "\n",
    "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
    "\n",
    "Alguns conselhos úteis:\n",
    "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
    "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
    "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3twP0YJC4jmJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnyhJZtTRNMx"
   },
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qlIOVCajPWcU"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w9f3PfifAwpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 17:26:16 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:26:00.0  On |                  N/A |\n",
      "|  0%   35C    P8    18W / 175W |    398MiB /  8192MiB |     15%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       871      G   /usr/lib/Xorg                     153MiB |\n",
      "|    0   N/A  N/A      5544      G   ...AAAAAAAAA= --shared-files       84MiB |\n",
      "|    0   N/A  N/A     11369      G   /usr/lib/firefox/firefox          157MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "whTCe2i7AtoV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZfxgV2DUk58"
   },
   "source": [
    "## Implementação do MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n_xhKm1EZ3bQ"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "\n",
    "def tokenize(text: str, tokenizer):\n",
    "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
    "\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
    "        # Escreva seu código aqui\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_size = context_size\n",
    "        self._dataset_tokenizer()\n",
    "        \n",
    "    def _dataset_tokenizer(self):\n",
    "        \n",
    "        arrays = np.concatenate([sliding_window_view(tokenize(text, self.tokenizer), self.context_size+1)\n",
    "                                 for text in self.texts]) \n",
    "        \n",
    "        self.data_ids = torch.LongTensor(arrays[:, :-1])\n",
    "        self.target_ids = torch.LongTensor(arrays[:, -1])\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Escreva seu código aqui\n",
    "        return len(self.target_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Escreva seu código aqui\n",
    "        return self.data_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wew-gFbWeBTq"
   },
   "source": [
    "## Teste se sua implementação do MyDataset está correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8r7jBFFUeApe"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8r7jBFFUeApe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    }
   ],
   "source": [
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
    "assert len(dummy_dataset) == 5\n",
    "print('passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[ 3396, 10303,   125],\n",
    "     [ 1660,  5971,   785],\n",
    "     [ 5971,   785,   125],\n",
    "     [  785,   125,  1847],\n",
    "     [  125,  1847, 13779]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
    "\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LfrHHouleJ0"
   },
   "source": [
    "# Carregamento do dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2vFWjsSkmop"
   },
   "source": [
    "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGlN1WqrXPA6"
   },
   "outputs": [],
   "source": [
    "#!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gxa_4gmiA-wE"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "valid_examples = 100\n",
    "test_examples = 100\n",
    "texts = open('sample_brwac.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gxa_4gmiA-wE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating for debugging purposes.\n"
     ]
    }
   ],
   "source": [
    "print('Truncating for debugging purposes.')\n",
    "texts = texts[:500]  \n",
    "\n",
    "training_texts = texts[:-(valid_examples + test_examples)]\n",
    "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
    "test_texts = texts[-test_examples:]\n",
    "\n",
    "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KCSGJ5m7py4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 406905\n",
      "valid examples: 135562\n",
      "test examples: 136690\n"
     ]
    }
   ],
   "source": [
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hGaAjYDfWdd1"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            context_size (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "        \"\"\"\n",
    "        # Escreva seu código aqui.\n",
    "        \n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, 64)        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, context_size)\n",
    "        \"\"\"\n",
    "        # Escreva seu código aqui.\n",
    "        input_embeddings = self.embeddings.weight[inputs.flatten()]\n",
    "        \n",
    "        return input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm6_PTH2i98e"
   },
   "source": [
    "## Teste o modelo com um exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "RwnxfZlrZoT_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 64])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_size=128,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset, batch_size=2)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "model(sample_train_gpu).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20100,  2308,  3074,  1089,   481,   117,   146,  1189,   125],\n",
       "        [ 2308,  3074,  1089,   481,   117,   146,  1189,   125, 13254]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20100,  2308,  3074,  1089,   481,   117,   146,  1189,   125])\n",
      "tensor([ 2308,  3074,  1089,   481,   117,   146,  1189,   125, 13254])\n"
     ]
    }
   ],
   "source": [
    "for input_ in sample_train:\n",
    "    print(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3Vh6B-VkA01"
   },
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of model parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nhbUVsYnVAp"
   },
   "source": [
    "## Assert da Perplexidade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbMP8VAUncfX"
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def perplexity(logits, target):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity.\n",
    "    \"\"\"\n",
    "    # Escreva seu código aqui.\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "n_examples = 1000\n",
    "\n",
    "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "target_token_ids = target_token_ids.to(device)\n",
    "logits = model(sample_train_gpu)\n",
    "\n",
    "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
    "\n",
    "print(f'my perplexity:              {int(my_perplexity)}')\n",
    "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
    "\n",
    "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
    "print('Passou o no assert da perplexidade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiJtrsqPnE_l"
   },
   "source": [
    "## Laço de Treinamento e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIMSaY-UUGUE"
   },
   "outputs": [],
   "source": [
    "max_examples = 100_000_000\n",
    "eval_every_steps = 5000\n",
    "lr = 3e-5\n",
    "\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_size=256,\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train_step(input, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    logits = model(input.to(device))\n",
    "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input, target):\n",
    "    logits = model(input)\n",
    "    loss = nn.functional.cross_entropy(logits, target)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "while n_examples < max_examples:\n",
    "    for input, target in train_loader:\n",
    "        loss = train_step(input.to(device), target.to(device)) \n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(input.to(device), target.to(device))\n",
    "                    for input, target in validation_loader]))\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(input)  # Increment of batch size\n",
    "        step += 1\n",
    "        if n_examples >= max_examples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgdNymJdNPXP"
   },
   "source": [
    "## Avaliação final no dataset de teste\n",
    "\n",
    "\n",
    "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxN5YytzZ7Tn"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target in test_loader\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHvEs8mPszy_"
   },
   "source": [
    "## Teste seu modelo com uma sentença\n",
    "\n",
    "Escolha uma sentença gerada pelo modelo que ache interessante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CFElf4tsytW"
   },
   "outputs": [],
   "source": [
    "prompt = ?  # Ex: 'Eu gosto de comer pizza pois me faz'\n",
    "max_output_tokens = 10\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
    "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
    "    prompt = tokenizer.decode(input_ids)\n",
    "    print(prompt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Aula 7 - Exercício",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_huggingface",
   "language": "python",
   "name": "venv_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
